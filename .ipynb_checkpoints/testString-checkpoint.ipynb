{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim import utils\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import os\n",
    "import getEmbeddings2\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot.metrics as skplt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textClean(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return (text)\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    text = textClean(text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = \"A sequential model which is a linear stack of layers is used. The first layer is an LSTM layer with 300 memory units and it returns sequences. This is done to ensure that the next LSTM layer receives sequences and not just randomly scattered data. A dropout layer is applied after each LSTM layer to avoid overfitting of the model. Finally, we have the last layer as a fully connected layer with a ‘softmax’ activation and neurons equal to the number of unique characters, because we need to output one hot encoded result.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sequential model which is a linear stack of layers is used. The first layer is an LSTM layer with 300 memory units and it returns sequences. This is done to ensure that the next LSTM layer receives sequences and not just randomly scattered data. A dropout layer is applied after each LSTM layer to avoid overfitting of the model. Finally, we have the last layer as a fully connected layer with a ‘softmax’ activation and neurons equal to the number of unique characters, because we need to output one hot encoded result.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = cleanup(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sequential model linear stack layers used first layer lstm layer 300 memory units returns sequences done ensure next lstm layer receives sequences randomly scattered data dropout layer applied lstm layer avoid overfitting model finally last layer fully connected layer softmax activation neurons equal number unique characters need output one hot encoded result'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "x_train = []\n",
    "x_train.append(new_test.split())\n",
    "for word in x_train[-1]:\n",
    "    cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sequential',\n",
       "  'model',\n",
       "  'linear',\n",
       "  'stack',\n",
       "  'layers',\n",
       "  'used',\n",
       "  'first',\n",
       "  'layer',\n",
       "  'lstm',\n",
       "  'layer',\n",
       "  '300',\n",
       "  'memory',\n",
       "  'units',\n",
       "  'returns',\n",
       "  'sequences',\n",
       "  'done',\n",
       "  'ensure',\n",
       "  'next',\n",
       "  'lstm',\n",
       "  'layer',\n",
       "  'receives',\n",
       "  'sequences',\n",
       "  'randomly',\n",
       "  'scattered',\n",
       "  'data',\n",
       "  'dropout',\n",
       "  'layer',\n",
       "  'applied',\n",
       "  'lstm',\n",
       "  'layer',\n",
       "  'avoid',\n",
       "  'overfitting',\n",
       "  'model',\n",
       "  'finally',\n",
       "  'last',\n",
       "  'layer',\n",
       "  'fully',\n",
       "  'connected',\n",
       "  'layer',\n",
       "  'softmax',\n",
       "  'activation',\n",
       "  'neurons',\n",
       "  'equal',\n",
       "  'number',\n",
       "  'unique',\n",
       "  'characters',\n",
       "  'need',\n",
       "  'output',\n",
       "  'one',\n",
       "  'hot',\n",
       "  'encoded',\n",
       "  'result']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'sequential': 1,\n",
       "         'model': 2,\n",
       "         'linear': 1,\n",
       "         'stack': 1,\n",
       "         'layers': 1,\n",
       "         'used': 1,\n",
       "         'first': 1,\n",
       "         'layer': 7,\n",
       "         'lstm': 3,\n",
       "         '300': 1,\n",
       "         'memory': 1,\n",
       "         'units': 1,\n",
       "         'returns': 1,\n",
       "         'sequences': 2,\n",
       "         'done': 1,\n",
       "         'ensure': 1,\n",
       "         'next': 1,\n",
       "         'receives': 1,\n",
       "         'randomly': 1,\n",
       "         'scattered': 1,\n",
       "         'data': 1,\n",
       "         'dropout': 1,\n",
       "         'applied': 1,\n",
       "         'avoid': 1,\n",
       "         'overfitting': 1,\n",
       "         'finally': 1,\n",
       "         'last': 1,\n",
       "         'fully': 1,\n",
       "         'connected': 1,\n",
       "         'softmax': 1,\n",
       "         'activation': 1,\n",
       "         'neurons': 1,\n",
       "         'equal': 1,\n",
       "         'number': 1,\n",
       "         'unique': 1,\n",
       "         'characters': 1,\n",
       "         'need': 1,\n",
       "         'output': 1,\n",
       "         'one': 1,\n",
       "         'hot': 1,\n",
       "         'encoded': 1,\n",
       "         'result': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8304b51ae3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmost_common\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword_bank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mid_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_common\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword_bank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_words' is not defined"
     ]
    }
   ],
   "source": [
    "most_common = cnt.most_common(top_words + 1)\n",
    "word_bank = {}\n",
    "id_num = 1\n",
    "for word, freq in most_common:\n",
    "    word_bank[word] = id_num\n",
    "    id_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
